# Maximum number of documents to retrieve for a document set
max_documents=2000000
max_documents=${?OV_MAX_DOCUMENTS}

# Where Overview stores its (Lucene, for now) search indexes
search {
  baseDirectory: "search"
  baseDirectory: ${?OV_SEARCH_DIRECTORY}
}

# Maximum memory for each clustering.
# This will be a `java` "-Xmx" setting: e.g., "2000m"
clustering_memory=2500m
clustering_memory=${?OV_CLUSTERING_MEMORY}

# Number of documents to convert to PDF at once.
#
# Converting documents takes 100% CPU, so don't set this higher than the
# number of CPUs.
n_document_converters=1
n_document_converters=${?OV_N_DOCUMENT_CONVERTERS}

# Maximum memory for each PDF processing step. (They may be concurrent.)
#
# This will be a `java` "-Xmx" setting: e.g., "1000m"
#
# This doesn't include the memory used by Tesseract. Assume that's ~100mb tops.
pdf_memory=1400m
pdf_memory=${?OV_PDF_MEMORY}

# time, in milliseconds, to allow LibreOffice to convert a file to PDF. Anything
# longer is deemed a failure. (LibreOffice can hang on invalid input.)
#
# The JVM can get confused by oosplash. See the comments in `libre_office_path`
# to avoid collecting zombie processes when you hit this timeout.
document_conversion_timeout=300000

# maximum number of UTF-16 characters in a single document. Overview isn't
# designed to handle massive string sizes; if this value is too high, you may
# see an OutOfMemoryError when we fetch and tokenize 20 documents from the
# database.
max_n_chars_per_document=655360 # ought to be enough for anybody ;)

# Maximum number of half-ingested files to hold for recursion. For instance,
# when handling a zipfile, the "unzip" processor will feed its input back to
# the beginning of the pipeline. If there is no room in the pipeline to process
# that recursed input, the worker will stall.
max_ingest_recurse_buffer_length=10000

# How many Documents to create at a time while ingesting. A large number here
# means fewer database COMMITs, but each COMMIT is more expensive.
ingest_batch_size=100

# Maximum amount of time we'll wait for an ingest batch to accumulate before
# writing. A large duration means fewer database COMMITs, but waiting can
# slow down imports a little bit
ingest_batch_max_wait=100ms

# Path to tesseract binary (eg. /usr/local/bin/tesseract)
# Set via environment variable TESSERACT_PATH or add the appropriate value to the PATH
tesseract_path=tesseract
tesseract_path=${?OV_TESSERACT_PATH}

# Clustering algorithm to use. One of:
#  KMeans
#  ConnectedComponents
#  KMeansComponents <- default
clustering_alg = KMeansComponents

# Maximum token length to cluster
#
# Any token above this number of characters (not Unicode codepoints) will
# be discarded before we begin clustering.
max_clustering_token_length=40
max_clustering_token_length=${?OV_MAX_CLUSTERING_TOKEN_LENGTH}

# Maximum memory to spend when sorting documents by metadata field values.
# This is _multiplied_ by the number of concurrent sorts.
max_mb_per_sort=200
max_mb_per_sort=${?OV_MAX_MB_PER_SORT}
n_concurrent_sorts=2
n_concurrent_sorts=${?OV_N_CONCURRENT_SORTS}

akka {
  jvm-exit-on-fatal-error: on

  actor {
    provider: "akka.remote.RemoteActorRefProvider"
    guardian-supervisor-strategy: "com.overviewdocs.akkautil.FailFastSupervisorStrategyConfigurator"
  }

  remote {
    enabled-transports: [ "akka.remote.netty.tcp" ]
    retry-gate-closed-for: 1s

    netty.tcp {
      hostname: "localhost"
      hostname: ${?MESSAGE_BROKER_HOSTNAME}
      port: 9030
      port: ${?MESSAGE_BROKER_PORT}
    }
  }
}

# http://doc.akka.io/docs/akka/snapshot/scala/dispatchers.html
blocking-io-dispatcher {
  type = Dispatcher
  executor = "thread-pool-executor"
  thread-pool-executor {
    fixed-pool-size = 8
  }
  throughput = 1
}
